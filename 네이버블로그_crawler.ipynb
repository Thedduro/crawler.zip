{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3cfc01-b830-448e-9a84-699be15811b7",
   "metadata": {},
   "source": [
    "#### 1. 필요한 라이브러리 및 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a631cfc0-c256-4627-9eac-67edf836b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 데이터 조작과 분석 시 필요\n",
    "import re # 정규 표현식을 지원하여 패턴 매칭과 텍스트 조작 시 필요\n",
    "import time # 시간 측정 시 필요\n",
    "import random # 난수 생성 시 필요\n",
    "import requests # HTTP요청을 만들고 웹 서비스와 상호작용 시 필요\n",
    "from bs4 import BeautifulSoup as bs # HTML이나 XML 문서를 구문 분석하고 탐색하기 위해 필요\n",
    "from selenium import webdriver # 브라우저 자동화를 위해 필요\n",
    "from selenium.common.exceptions import UnexpectedAlertPresentException #예기치 않은 경고나 팝업 발생 시 예외 처리에 필요\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fe96f-2e45-4dc8-9737-4a4ee1756f7e",
   "metadata": {},
   "source": [
    "#### 2. 네이버 API로 블로그 url 추출(데이터 수집 개수 제한 있음) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e94c2880-190e-4ca6-899c-980087809925",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_url_list=[]\n",
    "\n",
    "# 네이버 developers에서 내 정보 확인\n",
    "client_id = \"INPUT_client ID\"\n",
    "client_secret = \"INPUT_client Secret\"\n",
    "\n",
    "query = urllib.parse.quote(\"INPUT_검색키워드\")\n",
    "\n",
    "#100개 이상 수집 시 오류 발생\n",
    "idx=0\n",
    "display=10 # 한번에 가져오는 페이지 개수 설정\n",
    "start=1\n",
    "end=100\n",
    "\n",
    "\n",
    "for idx in range(start,end,display):\n",
    "    url = \"https://openapi.naver.com/v1/search/blog?query=\" + query + \"&display=\" + str(display) + \"&start=\" + str(idx) # 네이버 블로그 검색 API 호출 URL 생성성\n",
    "    \n",
    "    # D요청 객체 생성 및 헤더 추가\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "\n",
    "    # API 요청 및 응답 확인\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "\n",
    "    if(rescode==200): # 정상 반응\n",
    "        response_body = response.read()\n",
    "        data=response_body.decode('utf-8')\n",
    "        data = json.loads(data)\n",
    "        links = [item['link'] for item in data['items']]\n",
    "        for link in links:\n",
    "            blog_url_list.append(link) # 블로그 포스트 url 저장\n",
    "        \n",
    "    else: # 예외처리\n",
    "        print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0250e6",
   "metadata": {},
   "source": [
    "#### 3. 크롤링으로 블로그 url 추출 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22575c7a",
   "metadata": {},
   "source": [
    "##### 3.1 검색 페이지 url 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePgNum(num): # 페이지 넘버링 함수\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num+1\n",
    "    else:\n",
    "        return num+29*(num-1)\n",
    "\n",
    "\n",
    "# 크롤링할 url 생성 함수\n",
    "def makeUrl(search,start_pg,end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query=\" + search + \"&start=\" + str(page)\n",
    "        print(\"생성url: \",url)\n",
    "        return url\n",
    "    \n",
    "    else:\n",
    "        urls= []\n",
    "        for i in range(start_pg,end_pg+1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?ssc=tab.blog.all&sm=tab_jum&query=\" + search + \"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        print(\"생성url: \",urls)\n",
    "        return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2b4c7",
   "metadata": {},
   "source": [
    "##### 3.2 검색 페이지 url 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd12c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색하고자 하는 키워드를 입력\n",
    "search = input(\"검색할 키워드를 입력해주세요 :\")\n",
    "\n",
    "# 네이버 블로그는 한 페이지 당 30개의 블로그 정보 확인 가능함 (예를 들어, 1 - 10페이지 입력 시 300개의 정보 크롤링 가능)\n",
    "# 검색 시작할 페이지 입력\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요 :\"))\n",
    "# 검색 종료할 페이지 입력\n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요 :\"))\n",
    "\n",
    "# naver url 생성\n",
    "search_urls = makeUrl(search,page,page2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c9dc8",
   "metadata": {},
   "source": [
    "##### 3.3 블로그 포스트 url 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 블로그 포스트 url 저장 리스트\n",
    "blog_url_list=[]\n",
    "\n",
    "for i in search_urls:\n",
    "    r=requests.get(i) # requests.get(url)로 요청\n",
    "    time.sleep(random.radint(1,3)) # 요청시간 랜덤 조정으로 크롤링 차단 방지\n",
    "\n",
    "    soup=bs(r.text, \"html.parser\") # html.parser로 파싱\n",
    "    \n",
    "    # 제목, 본문 텍스트 영역 추출\n",
    "    title_areas=soup.find_all(\"div\", {'class':'title_area'})\n",
    "    user_boxes = soup.find_all(\"div\", {'class':'user_box'})\n",
    "    \n",
    "    for url,pro in zip(title_areas, user_boxes):\n",
    "        if '광고' in pro.text: # 포스트 옆에 광고 표시가 떠있으면 제외\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            url_text = str(url)\n",
    "            pattern = r'href=\"([^\"]+)\"'\n",
    "            links = re.findall(pattern, url_text) # 정규식으로 링크 추출\n",
    "            blog_url_list.append(links)\n",
    "\n",
    "print(len(blog_url_list),' 개 블로그 url주소 수집') # 수집된 포스트 url 개수 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93552546",
   "metadata": {},
   "source": [
    "#### 4. url 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수집 후 blog_url_list에 중복된 url이 있는지 확인 후 중복 제거 (순서 보존)\n",
    "filtered_urls = [url for url in blog_url_list if 'blog.naver' in url[0]]\n",
    "unique_urls = list(dict.fromkeys(url[0] for url in filtered_urls))\n",
    "\n",
    "# 최종 블로그 url 목록 개수 출력 \n",
    "print(len(unique_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f77bebe0-506d-41e1-937f-8d3d126583a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url 파일 저장\n",
    "url = pd.DataFrame(unique_urls)\n",
    "url.to_csv('naverblog_url.csv',encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb55bd",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec31a8f",
   "metadata": {},
   "source": [
    "#### 5. 블로그 포스트 본문 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7181c7b-cf43-48df-a00c-ac577e4b69cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장한 url 파일 불러오기\n",
    "url = pd.read_csv('naverblog_url.csv') #url 저장위치, 파일명 입력\n",
    "blog_url_list = list(url['0']) # url 파일의 첫번째(0) 컬럼을 blog_url_list로 저장\n",
    "len(blog_url_list) # 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57f094aa-f146-4f91-b1bf-5239150468a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10개 블로그 크롤링 완료\n",
      "20개 블로그 크롤링 완료\n",
      "30개 블로그 크롤링 완료\n",
      "40개 블로그 크롤링 완료\n",
      "50개 블로그 크롤링 완료\n",
      "60개 블로그 크롤링 완료\n",
      "70개 블로그 크롤링 완료\n",
      "80개 블로그 크롤링 완료\n",
      "90개 블로그 크롤링 완료\n",
      "100개 블로그 크롤링 완료\n",
      "110개 블로그 크롤링 완료\n",
      "120개 블로그 크롤링 완료\n",
      "130개 블로그 크롤링 완료\n",
      "140개 블로그 크롤링 완료\n",
      "150개 블로그 크롤링 완료\n",
      "160개 블로그 크롤링 완료\n",
      "170개 블로그 크롤링 완료\n",
      "180개 블로그 크롤링 완료\n",
      "190개 블로그 크롤링 완료\n",
      "200개 블로그 크롤링 완료\n",
      "210개 블로그 크롤링 완료\n",
      "220개 블로그 크롤링 완료\n",
      "230개 블로그 크롤링 완료\n",
      "240개 블로그 크롤링 완료\n",
      "250개 블로그 크롤링 완료\n",
      "260개 블로그 크롤링 완료\n",
      "270개 블로그 크롤링 완료\n",
      "280개 블로그 크롤링 완료\n",
      "290개 블로그 크롤링 완료\n",
      "300개 블로그 크롤링 완료\n",
      "310개 블로그 크롤링 완료\n",
      "320개 블로그 크롤링 완료\n",
      "330개 블로그 크롤링 완료\n",
      "340개 블로그 크롤링 완료\n",
      "350개 블로그 크롤링 완료\n",
      "360개 블로그 크롤링 완료\n",
      "370개 블로그 크롤링 완료\n",
      "380개 블로그 크롤링 완료\n",
      "390개 블로그 크롤링 완료\n",
      "400개 블로그 크롤링 완료\n",
      "410개 블로그 크롤링 완료\n",
      "420개 블로그 크롤링 완료\n",
      "430개 블로그 크롤링 완료\n",
      "440개 블로그 크롤링 완료\n",
      "450개 블로그 크롤링 완료\n",
      "460개 블로그 크롤링 완료\n",
      "470개 블로그 크롤링 완료\n",
      "480개 블로그 크롤링 완료\n",
      "490개 블로그 크롤링 완료\n",
      "500개 블로그 크롤링 완료\n",
      "510개 블로그 크롤링 완료\n",
      "520개 블로그 크롤링 완료\n",
      "530개 블로그 크롤링 완료\n",
      "540개 블로그 크롤링 완료\n",
      "550개 블로그 크롤링 완료\n",
      "560개 블로그 크롤링 완료\n",
      "570개 블로그 크롤링 완료\n",
      "580개 블로그 크롤링 완료\n",
      "590개 블로그 크롤링 완료\n",
      "600개 블로그 크롤링 완료\n",
      "610개 블로그 크롤링 완료\n",
      "620개 블로그 크롤링 완료\n",
      "630개 블로그 크롤링 완료\n",
      "640개 블로그 크롤링 완료\n",
      "650개 블로그 크롤링 완료\n",
      "660개 블로그 크롤링 완료\n",
      "670개 블로그 크롤링 완료\n",
      "680개 블로그 크롤링 완료\n",
      "690개 블로그 크롤링 완료\n",
      "700개 블로그 크롤링 완료\n",
      "710개 블로그 크롤링 완료\n",
      "720개 블로그 크롤링 완료\n",
      "730개 블로그 크롤링 완료\n",
      "740개 블로그 크롤링 완료\n",
      "750개 블로그 크롤링 완료\n",
      "760개 블로그 크롤링 완료\n",
      "770개 블로그 크롤링 완료\n",
      "780개 블로그 크롤링 완료\n",
      "790개 블로그 크롤링 완료\n",
      "800개 블로그 크롤링 완료\n",
      "810개 블로그 크롤링 완료\n",
      "820개 블로그 크롤링 완료\n",
      "830개 블로그 크롤링 완료\n",
      "840개 블로그 크롤링 완료\n",
      "850개 블로그 크롤링 완료\n",
      "860개 블로그 크롤링 완료\n",
      "870개 블로그 크롤링 완료\n",
      "880개 블로그 크롤링 완료\n",
      "890개 블로그 크롤링 완료\n",
      "900개 블로그 크롤링 완료\n",
      "910개 블로그 크롤링 완료\n",
      "920개 블로그 크롤링 완료\n",
      "930개 블로그 크롤링 완료\n",
      "940개 블로그 크롤링 완료\n",
      "950개 블로그 크롤링 완료\n",
      "960개 블로그 크롤링 완료\n",
      "970개 블로그 크롤링 완료\n",
      "980개 블로그 크롤링 완료\n",
      "990개 블로그 크롤링 완료\n",
      "네이버 블로그 아닌 url:  https://dumak.tistory.com/198\n"
     ]
    }
   ],
   "source": [
    "no_naver_blog=[] # 네이버 블로그 외 다른 주소 필터링\n",
    "\n",
    "start = time.time() # 소요 시간 측정\n",
    "\n",
    "# 수집 항목\n",
    "time_list = [] # 작성 시간\n",
    "review_list = [] # 본문\n",
    "url_list = [] # url 주소\n",
    "count = 0 # 진행 상황을 파악하기 위한 크롤링 개수 카운팅\n",
    "\n",
    "driver = webdriver.Chrome() \n",
    "\n",
    "for url in blog_url_list: \n",
    "    if 'naver' in url: \n",
    "\n",
    "        count += 1\n",
    "        driver.get(url)\n",
    "        time.sleep(random.radint(1,3)) # 요청시간 랜덤 조정으로 크롤링 차단 방지\n",
    "\n",
    "        try: # 네이버 블로그 크롤링\n",
    "            driver.switch_to.frame('mainFrame') # 네이버는 여러 mainFrame으로 구성되어 있어서, 원하는 프레임으로 전환\n",
    "            time.sleep(0.7) \n",
    "            soup = bs(driver.page_source, 'lxml') # 현재 페이지의 HTML 소스 파싱\n",
    "            \n",
    "            postview = soup.find('div', attrs={'id': re.compile('post-view.+')}).get_text() # 게시글 요소 텍스트 추출\n",
    "            reg = re.compile(r'[\\s+]') # 공백 처리 패턴 정의 (\\s 공백, + 1개 이상 연속) \n",
    "            postview_reg = reg.sub(' ',postview) # 한 칸 공백으로 대체\n",
    "            \n",
    "            try:\n",
    "                # 게시물 작성 시간\n",
    "                timeline = driver.find_element(\"xpath\", \"//span[@class='se_publishDate pcol2']\") # 시간 요소 저장(유형 1)\n",
    "            except:\n",
    "                timeline = driver.find_element(\"xpath\", \"//p[@class='date fil5 pcol2 _postAddDate']\") # 시간 요소 저장(유형2)\n",
    "            timeline = timeline.text # 저장\n",
    "             \n",
    "            # 수집한 내용 각 리스트 추가\n",
    "            time_list.append(timeline)\n",
    "            review_list.append(postview_reg)\n",
    "            url_list.append(url)\n",
    "            time.sleep(random.uniform(0.5,1.3))\n",
    "\n",
    "        except UnexpectedAlertPresentException: # 예외처리\n",
    "            print('Error extracting data')\n",
    "            pass\n",
    "    else: # 제외 링크 확인용\n",
    "        no_naver_blog.append(url)\n",
    "        print('네이버 블로그 아닌 url: ',url)\n",
    "        \n",
    "    if count%10 ==0: # 진행 상황 확인용\n",
    "        print('{}개 블로그 크롤링 완료'.format(count)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166cef13",
   "metadata": {},
   "source": [
    "#### 6. 데이터 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e473b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'time' : time_list, # 시간\n",
    "        'review' : review_list, # 게시글\n",
    "        'url': url_list} # url\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates() # 중복제거\n",
    "\n",
    "print('크롤링 시간: ', time.time()-start)\n",
    "print('크롤링 개수:', len(review_list))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1813312-c588-467b-b82c-78ef39a7d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('naverblog_result.csv', encoding = \"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
