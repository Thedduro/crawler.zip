{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441f9f8b",
   "metadata": {},
   "source": [
    "##### 1. 필요한 라이브러리 및 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506a6648-de56-43be-8ce7-ecd26428d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a706d9",
   "metadata": {},
   "source": [
    "##### 2. url 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a5091d-0d97-4b49-9970-42be31ab8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePgNum(num): # 페이지 넘버링 함수\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num+1\n",
    "    else:\n",
    "        return num+9*(num-1)\n",
    "\n",
    "\n",
    "# 크롤링할 url 생성 함수\n",
    "def makeUrl(search,start_pg,end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&sort=1&start=\" + str(page)\n",
    "        return url\n",
    "    else:\n",
    "        urls= []\n",
    "        for i in range(start_pg,end_pg+1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&sort=1&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        return urls # 전체 url 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c015a0fc-a69b-41bc-894c-75c8ff48cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 뉴스 링크 추출 함수\n",
    "def get_naverlink(search_urls):\n",
    "\n",
    "    naverlink_list=[]\n",
    "    \n",
    "    for i in search_urls:\n",
    "        r=requests.get(i) # requests.get(url)로 요청\n",
    "        time.sleep(random.randint(1,3)) # 요청시간 랜덤 조정으로 크롤링 차단 방지\n",
    "    \n",
    "        soup=BeautifulSoup(r.text, \"html.parser\") # html.parser로 파싱\n",
    "        content_items= soup.select(\".info_group\") # select로 태그 선택\n",
    "        content_items=str(content_items) # 문자열 변환\n",
    "    \n",
    "        pattern = r'href=\"([^\"]+)\"'\n",
    "        links = re.findall(pattern, content_items) # 링크만 추출\n",
    "    \n",
    "        naver_word='news.naver.com' \n",
    "    \n",
    "        for j in links:\n",
    "            if naver_word in j:\n",
    "                naverlink_list.append(j)\n",
    "    \n",
    "    return naverlink_list # 전체 네이버 뉴스 링크 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fc00a",
   "metadata": {},
   "source": [
    "##### 3. 뉴스 기사 크롤러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2276df77-06fc-49bf-82db-2ce88d61f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naver 기사 제목, 본문, 시간 가져오기\n",
    "\n",
    "def get_title_content(naverlink_list):\n",
    "    \n",
    "    # ConnectionError방지\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"ko-KR,ko;q=0.9\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"TE\": \"Trailers\",\n",
    "            \"DNT\": \"1\",\n",
    "        }\n",
    "    \n",
    "    contents=[] # 본문\n",
    "    titles=[] # 제목\n",
    "    times=[] # 작성 시간\n",
    "\n",
    "    for i in naverlink_list:\n",
    "        r = requests.get(i,headers=headers)\n",
    "        time.sleep(random.radint(1,3)) # 요청시간 랜덤 조정으로 크롤링 차단 방지\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        content = soup.find('div',{'class': 'newsct_article _article_body'})\n",
    "        title =soup.find('div',{'class': 'media_end_head_title'})\n",
    "        time=soup.find('div',{'class': 'media_end_head_info_datestamp_bunch'})\n",
    "    \n",
    "        #제목 텍스트만 가져오기\n",
    "        title=title.text\n",
    "        title=title.replace(\"\\n\",\"\")\n",
    "        titles.append(title) \n",
    "         \n",
    "        # 기사 텍스트만 가져오기\n",
    "        content=content.text.replace(\"\\n\",\"\")\n",
    "        contents.append(content)\n",
    "        \n",
    "        # 시간 정보 텍스트만\n",
    "        time=time.text\n",
    "        time=time.replace(\"\\n\",\"\")\n",
    "        time=time.replace(\"입력\",\"\")\n",
    "        time=time[:11]\n",
    "        times.append(time)\n",
    "        \n",
    "    return contents, titles, times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524994f9",
   "metadata": {},
   "source": [
    "##### 4. url 생성 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05840b27-7660-47b6-ad5b-13ff25337ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색할 키워드를 입력해주세요: 반려견 건식사료 영향\n",
      "\n",
      "크롤링할 시작 페이지를 입력해주세요.: 1\n",
      "\n",
      "크롤링할 종료 페이지를 입력해주세요.: 50\n"
     ]
    }
   ],
   "source": [
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "\n",
    "#검색 시작할 페이지 입력\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요.:\"))\n",
    "#검색 종료할 페이지 입력\n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요.:\"))\n",
    "\n",
    "# naver url 생성\n",
    "search_urls = makeUrl(search,page,page2)\n",
    "\n",
    "# naver news url 추출\n",
    "naverlink_list = get_naverlink(search_urls) # 네이버 뉴스 링크 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc8768",
   "metadata": {},
   "source": [
    "##### 5. 크롤링 작동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44b4204c-f399-49aa-8f53-5ff6abd8da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "naverlink_list = set(naverlink_list) # 뉴스 링크 중복 제거\n",
    "naverlink_list = list(naverlink_list) # 다시 리스트 형태로 변환\n",
    "\n",
    "contents, titles, times = get_title_content(naverlink_list) # 크롤러 호출\n",
    "news_df = pd.DataFrame({'Title':titles,'Time': times,'Link':naverlink_list,'Content':contents}) # 데이터프레임으로 크롤링 데이터 구조화\n",
    "\n",
    "print(len(news_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94416539",
   "metadata": {},
   "source": [
    "##### 6. 데이터 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770abf3",
   "metadata": {},
   "source": [
    "- 여러 검색 키워드로 크롤링 했을 경우 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a941871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_df=pd.concat([news_dfn, news_dfn, news_dfn])\n",
    "# news_df=news_df.drop_duplicates()\n",
    "# print(len(news_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d356f3",
   "metadata": {},
   "source": [
    "news_df.to_csv('NaverNews_content.csv', index=False, encoding='utf-8-sig') # 한국어 저장이 안정적인 인코딩 방식"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
